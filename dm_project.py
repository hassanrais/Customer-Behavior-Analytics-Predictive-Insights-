# -*- coding: utf-8 -*-
"""DM_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10La2vMQ1vJijp-ReC6XLDKsbAMGvSpyj

# **Customer Behavior Analytics & Predictive Insights**
A Project by:
* Hassan Rais - 2022212
* M.Hamza Zaidi - 2022379

DS-341

Instructor: Dr. Ayaz Umer
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans

from mlxtend.frequent_patterns import apriori, association_rules

df = pd.read_csv('online_shoppers_intention.csv')

df['Month'] = LabelEncoder().fit_transform(df['Month'])
df['VisitorType'] = LabelEncoder().fit_transform(df['VisitorType'])
df['Weekend'] = df['Weekend'].astype(int)
df['Revenue'] = df['Revenue'].astype(int)

# If Revenue is in 'Purchase' / 'NoPurchase' form, convert back to 0/1
df['Revenue'] = df['Revenue'].map({'NoPurchase': 0, 'Purchase': 1})
df['Weekend'] = df['Weekend'].map({'Weekday': 0, 'Weekend': 1})
df['Revenue'] = df['Revenue'].astype(int)
df['Weekend'] = df['Weekend'].astype(int)


# --- 1. Basic Overview ---
print("üîç Dataset Info:")
print(df.info())
print("\nüìä Basic Statistics:")
print(df.describe(include='all'))

# --- 3. Distribution of Target Variable ---
print("\nüéØ Target Variable Distribution (Revenue):")
print(df['Revenue'].value_counts(normalize=True))

plt.figure(figsize=(6,4))
sns.countplot(data=df, x='Revenue')
plt.title('Revenue Distribution (0 = No Purchase, 1 = Purchase)')
plt.xlabel('Revenue')
plt.ylabel('Count')
plt.show()

# --- 4. Categorical Feature Distributions ---
categorical = ['Month', 'VisitorType', 'Weekend']

for col in categorical:
    plt.figure(figsize=(7,4))
    sns.countplot(data=df, x=col, hue='Revenue')
    plt.title(f'{col} vs Revenue')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# --- 5. Numerical Feature Distributions ---
numerical = ['Administrative_Duration', 'Informational_Duration', 'ProductRelated_Duration',
             'BounceRates', 'ExitRates', 'PageValues']

for col in numerical:
    plt.figure(figsize=(6,4))
    sns.histplot(data=df, x=col, hue='Revenue', kde=True, bins=50)
    plt.title(f'Distribution of {col} by Revenue')
    plt.show()

# --- 8. Aggregated Views ---
grouped = df.groupby('Revenue').mean(numeric_only=True)
print("\nüìà Mean of Numerical Features by Revenue:")
print(grouped.T.sort_values(by=1, ascending=False))  # sort by buyers

# --- 9. Revenue by Month (Chronological Order) ---
month_order = ['Feb', 'Mar', 'May', 'June', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
df['Month'] = pd.Categorical(df['Month'], categories=month_order, ordered=True)

plt.figure(figsize=(10,4))
sns.countplot(data=df, x='Month', hue='Revenue', order=month_order)
plt.title('Monthly Sessions by Revenue')
plt.show()

# --- 4. Categorical Features vs Revenue ---
categorical_features = ['OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend']

for col in categorical_features:
    plt.figure(figsize=(8, 4))
    sns.countplot(data=df, x=col, hue='Revenue')
    plt.title(f'{col} Distribution by Revenue')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# --- 6. Correlation Analysis ---
corr = df.corr(numeric_only=True)

plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

print("\nüîó Features most correlated with Revenue:")
print(corr['Revenue'].sort_values(ascending=False))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules

# Load and prepare dataset
df = pd.read_csv('online_shoppers_intention.csv')
df['PageValueGroup'] = pd.cut(df['PageValues'], bins=[-1, 0, df['PageValues'].max()], labels=['NoValue', 'HasValue'])
df['VisitorType'] = df['VisitorType'].astype(str)
df['Weekend'] = df['Weekend'].map({False: 'Weekday', True: 'Weekend'})
df['Revenue'] = df['Revenue'].map({False: 'NoPurchase', True: 'Purchase'})

# Transactional format
df_trans = pd.get_dummies(df[['PageValueGroup', 'VisitorType', 'Weekend', 'Revenue']])

# --- Apriori ---
frequent_apriori = apriori(df_trans, min_support=0.05, use_colnames=True)
rules_apriori_full = association_rules(frequent_apriori, metric='confidence', min_threshold=0.5)

if not rules_apriori_full.empty:
    top_apriori = rules_apriori_full.sort_values(by='confidence', ascending=False).head(5)
    rules_apriori = rules_apriori_full[['confidence']].reset_index(drop=True)
    rules_apriori['Method'] = 'Apriori'
    rules_apriori['Rule_ID'] = rules_apriori.index
else:
    top_apriori = pd.DataFrame(columns=['antecedents', 'consequents', 'support', 'confidence', 'lift'])
    rules_apriori = pd.DataFrame(columns=['confidence', 'Method', 'Rule_ID'])

# --- FP-Growth ---
frequent_fpgrowth = fpgrowth(df_trans, min_support=0.05, use_colnames=True)
rules_fpgrowth_full = association_rules(frequent_fpgrowth, metric='confidence', min_threshold=0.5)

if not rules_fpgrowth_full.empty:
    top_fpgrowth = rules_fpgrowth_full.sort_values(by='confidence', ascending=False).head(5)
    rules_fpgrowth = rules_fpgrowth_full[['confidence']].reset_index(drop=True)
    rules_fpgrowth['Method'] = 'FP-Growth'
    rules_fpgrowth['Rule_ID'] = rules_fpgrowth.index
else:
    top_fpgrowth = pd.DataFrame(columns=['antecedents', 'consequents', 'support', 'confidence', 'lift'])
    rules_fpgrowth = pd.DataFrame(columns=['confidence', 'Method', 'Rule_ID'])

# --- Display Top 5 Rules ---
print("üîπ Top 5 Apriori Rules:")
print(top_apriori[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

print("\nüî∏ Top 5 FP-Growth Rules:")
print(top_fpgrowth[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# --- Combine Confidence Data for Visualization ---
comparison_df = pd.concat([rules_apriori, rules_fpgrowth], ignore_index=True)

# --- 1. Line Plot: Confidence Across Rules ---
plt.figure(figsize=(10, 5))
sns.lineplot(data=comparison_df, x='Rule_ID', y='confidence', hue='Method', marker='o')
plt.title('Confidence Comparison Across Rules (Line Plot)')
plt.xlabel('Rule Index')
plt.ylabel('Confidence')
plt.grid(True)
plt.show()

# --- 2. Strip Plot: Confidence Distribution ---
plt.figure(figsize=(8, 5))
sns.stripplot(data=comparison_df, x='Method', y='confidence', jitter=True, alpha=0.7)
plt.title('Confidence Distribution of Rules by Method (Strip Plot)')
plt.ylabel('Confidence')
plt.show()

from mlxtend.frequent_patterns import association_rules



# Generate rules again if needed
rules_apriori = association_rules(frequent_apriori, metric='confidence', min_threshold=0.3)
rules_fpgrowth = association_rules(frequent_fpgrowth, metric='confidence', min_threshold=0.3)

# Filter only those rules where a purchase happened
purchase_rules_apriori = rules_apriori[rules_apriori['consequents'].astype(str).str.contains("Revenue_Purchase")]
purchase_rules_fpgrowth = rules_fpgrowth[rules_fpgrowth['consequents'].astype(str).str.contains("Revenue_Purchase")]

# Sort by confidence or lift to find strong co-purchasing patterns
top_purchase_rules = purchase_rules_apriori.sort_values(by='confidence', ascending=False).head(10)

# Display results
print("üí° Top Co-Purchasing Patterns (Apriori, leading to Purchase):")
print(top_purchase_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

from mlxtend.frequent_patterns import association_rules

# Generate rules if not already created
rules_fpgrowth = association_rules(frequent_fpgrowth, metric='confidence', min_threshold=0.3)

# Filter rules where the consequent indicates a purchase
purchase_rules_fpgrowth = rules_fpgrowth[rules_fpgrowth['consequents'].astype(str).str.contains("Revenue_Purchase")]

# Sort by confidence or lift to identify the strongest patterns
top_purchase_rules_fp = purchase_rules_fpgrowth.sort_values(by='confidence', ascending=False).head(10)

# Display results
print("üí° Top Co-Purchasing Patterns (FP-Growth, leading to Purchase):")
print(top_purchase_rules_fp[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# --- Load and Clean Dataset ---
df = pd.read_csv('online_shoppers_intention.csv')

# Encode categorical features
df['Month'] = LabelEncoder().fit_transform(df['Month'])
df['VisitorType'] = LabelEncoder().fit_transform(df['VisitorType'])
df['Weekend'] = df['Weekend'].astype(bool).astype(int)
df['Revenue'] = df['Revenue'].astype(bool).astype(int)

# Drop features that may leak purchase info
leak_features = ['PageValues', 'BounceRates', 'ExitRates']
X = df.drop(columns=['Revenue'] + leak_features)
y = df['Revenue']

# Stratified Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# Scale Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Try K values from 1 to 30
k_values = range(1, 31)
accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train_scaled, y_train)
    preds = knn.predict(X_test_scaled)
    acc = accuracy_score(y_test, preds)
    accuracies.append(acc)

# Plot the elbow graph
plt.figure(figsize=(10, 5))
plt.plot(k_values, accuracies, marker='o')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()


# --- Decision Tree ---
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train_scaled, y_train)
dt_preds = dt.predict(X_test_scaled)
print("\nüü® Decision Tree Report:")
print(classification_report(y_test, dt_preds, target_names=["No Purchase", "Purchase"]))

# --- Naive Bayes ---
nb = GaussianNB()
nb.fit(X_train_scaled, y_train)
nb_preds = nb.predict(X_test_scaled)
print("\nüü¶ Naive Bayes Report:")
print(classification_report(y_test, nb_preds, target_names=["No Purchase", "Purchase"]))

# --- K-Nearest Neighbors ---
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train_scaled, y_train)
knn_preds = knn.predict(X_test_scaled)
print("\nüü• K-Nearest Neighbors Report:")
print(classification_report(y_test, knn_preds, target_names=["No Purchase", "Purchase"]))

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Encode categorical features
df['Month'] = LabelEncoder().fit_transform(df['Month'])
df['VisitorType'] = LabelEncoder().fit_transform(df['VisitorType'])
df['Weekend'] = df['Weekend'].astype(bool).astype(int)

# Remove target
X = df.drop(columns=['Revenue'])

# Scale
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
df['Cluster'] = kmeans.fit_predict(X_scaled)

# PCA for visualization
pca = PCA(n_components=2)
components = pca.fit_transform(X_scaled)
df['PC1'] = components[:, 0]
df['PC2'] = components[:, 1]

# Plot clusters
plt.figure(figsize=(8,5))
sns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set2')
plt.title('Customer Segmentation via KMeans Clustering')
plt.show()

import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
import matplotlib.pyplot as plt


df = pd.read_csv('online_shoppers_intention.csv')
df['Month'] = LabelEncoder().fit_transform(df['Month'])
df['VisitorType'] = LabelEncoder().fit_transform(df['VisitorType'])
df['Weekend'] = df['Weekend'].astype(bool).astype(int)

# Drop target and scale features
X = df.drop(columns=['Revenue'])
X_scaled = StandardScaler().fit_transform(X)

# Compute linkage and plot dendrogram
Z = linkage(X_scaled, method='ward')
plt.figure(figsize=(12, 5))
dendrogram(Z, truncate_mode='lastp', p=30, leaf_rotation=45, leaf_font_size=10)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance')
plt.tight_layout()
plt.show()

# Assign clusters (e.g., 3)
df['HC_Cluster'] = fcluster(Z, t=3, criterion='maxclust')